# Attributions

The table below is a record of attributions, listing the original authors of the used external resources within this
project.

# Source code

The following source code is contained in this project.

| **URL**                             | **License**    |
| ----------------------------------- | -------------- |
| https://github.com/tiangolo/fastapi | MIT License    |
| https://github.com/encode/uvicorn   | BSD 3-Clause   |
| https://github.com/opencv/opencv    | Apache License |

# Media

The video resources

- _resources/video1_1.avi_
- _resources/video1_2.avi_
- _resources/video1_3.avi_
- _resources/video1_4.avi_
- _resources/video1_5.avi_

are taken from the [WiseNET dataset](https://data.4tu.nl/articles/_/12714416/1).

```
Roberto Marroquin, Julien Dubois, Christophe Nicolle,

WiseNET: An indoor multi-camera multi-space dataset with contextual information and annotations for people detection and
tracking,

Data in Brief,
Volume 27,
2019,
104654,
ISSN 2352-3409,

https://doi.org/10.1016/j.dib.2019.104654.

(https://www.sciencedirect.com/science/article/pii/S2352340919310091)

Abstract: Nowadays, camera networks are part of our every-day life environments, consequently, they represent a massive
source of information for monitoring human activities and to propose new services to the building users. To perform
human activity monitoring, people must be detected and the analysis has to be done according to the information relative
to the environment and the context. Available multi-camera datasets furnish videos with few (or none) information of the
environment where the network was deployed. The proposed dataset provides multi-camera multi-space video sets along with
the complete contextual information of the environment. The dataset regroups 11 video sets (composed of 62 single
videos) recorded using 6 indoor cameras deployed on multiple spaces. The video sets represent more than 1Â h of video
footage, include 77 people tracks and captured different human actions such as walking around, standing/sitting,
motionless, entering/leaving a space and group merging/splitting. Moreover, each video has been manually and
automatically annotated to include people detection and tracking meta-information. The automatic people detection
annotations were obtained by using different complexity and robustness detectors, from machine learning to state-of-art
deep Convolutional Neural Network (CNN) models. Concerning the contextual information, the Industry Foundation Classes
(IFC) file that represents the environment's Building Information Modeling (BIM) data is also provided. The BIM/IFC file
describes the complete structure of the environment, it's topology and the elements contained in it. To our knowledge,
the WiseNET dataset is the first to provide a set of videos along with the complete information of the environment. The
WiseNET dataset is publicly available at https://doi.org/10.4121/uuid:c1fb5962-e939-4c51-bfd5-eac6f2935d44, as well as
at the project's website http://wisenet.checksem.fr/#/dataset.

Keywords: Indoor multi-camera multi-space dataset; People detection; People tracking; BIM; Contextual information
```
